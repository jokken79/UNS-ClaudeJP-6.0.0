"""
Import 1,156 candidates WITH PHOTOS from JSON file
=====================================================

This script imports candidates from all_candidates_with_photos.json
with complete field mapping and photo_data_url support.

Features:
- Batch processing (100 records per commit)
- Progress reporting
- Error handling with detailed logs
- Photo data preservation
- Duplicate detection

Usage:
    docker exec -it uns-claudejp-600-backend-1 bash
    cd /app
    python scripts/import_candidates_with_photos.py
"""

import sys
import json
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, Any

sys.path.insert(0, '/app')

from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from app.core.database import SessionLocal
from app.models.models import Candidate, CandidateStatus

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_date(date_str):
    """Parse date from various formats"""
    if not date_str or date_str == 'None' or date_str == '':
        return None

    try:
        return datetime.fromisoformat(date_str).date()
    except (ValueError, TypeError, AttributeError):
        pass

    try:
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y']:
            try:
                return datetime.strptime(str(date_str), fmt).date()
            except (ValueError, TypeError):
                continue
    except (ValueError, TypeError, AttributeError):
        pass

    return None


def parse_int(value):
    """Parse integer safely"""
    if not value or str(value).strip() == '':
        return None
    try:
        return int(float(value))
    except (ValueError, TypeError):
        return None


def parse_float(value):
    """Parse float safely"""
    if not value or str(value).strip() == '':
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def parse_bool(value):
    """Convert Japanese yes/no to boolean"""
    if not value:
        return None
    value_str = str(value).strip()
    return value_str in ['Êúâ', '‚óã', '„ÅØ„ÅÑ', 'Yes', '1', 'TRUE']


def normalize_percentage(value):
    """Normalize percentage values to standard format (0-100)"""
    if not value:
        return None
    value_str = str(value).strip().replace('%', '').strip()
    try:
        num = float(value_str)
        if num > 100:
            return '100%'
        elif num < 0:
            return '0%'
        else:
            return f'{int(num)}%'
    except (ValueError, TypeError):
        return value_str


def import_candidate(db: Session, data: dict, stats: dict) -> bool:
    """Import a single candidate with complete field mapping"""

    try:
        # Extract rirekisho_id - primary unique identifier
        rirekisho_id_raw = data.get('Â±•Ê≠¥Êõ∏ID', data.get('Â±•Ê≠¥Êõ∏‚Ññ'))

        if not rirekisho_id_raw or str(rirekisho_id_raw) == 'None':
            stats['errors'] += 1
            logger.error(f"  Missing rirekisho_id for: {data.get('Ê∞èÂêç', 'Unknown')}")
            return False

        rirekisho_id = str(rirekisho_id_raw).strip()

        # Check if already exists
        existing = db.query(Candidate).filter(Candidate.rirekisho_id == rirekisho_id).first()
        if existing:
            stats['duplicates'] += 1
            logger.debug(f"  Duplicate: {rirekisho_id}")
            return False

        # Get photo data
        photo_data_url = data.get('photo_data_url')
        if photo_data_url:
            stats['with_photo'] = stats.get('with_photo', 0) + 1

        # Create candidate with COMPLETE mapped fields
        candidate = Candidate(
            rirekisho_id=rirekisho_id,

            # === BASIC INFO (Âü∫Êú¨ÊÉÖÂ†±) ===
            full_name_kanji=data.get('Ê∞èÂêç'),
            full_name_kana=data.get('„Éï„É™„Ç¨„Éä'),
            full_name_roman=data.get('Ê∞èÂêçÔºà„É≠„Éº„ÉûÂ≠ó)'),
            gender=data.get('ÊÄßÂà•'),
            date_of_birth=parse_date(data.get('ÁîüÂπ¥ÊúàÊó•')),
            nationality=data.get('ÂõΩÁ±ç'),
            marital_status=data.get('ÈÖçÂÅ∂ËÄÖ'),
            hire_date=parse_date(data.get('ÂÖ•Á§æÊó•')),
            reception_date=parse_date(data.get('Âèó‰ªòÊó•')),
            arrival_date=parse_date(data.get('Êù•Êó•')),

            # === PHOTO (ÂÜôÁúü) ===
            photo_data_url=photo_data_url,

            # === CONTACT (ÈÄ£Áµ°ÂÖà) ===
            phone=data.get('ÈõªË©±Áï™Âè∑'),
            mobile=data.get('Êê∫Â∏ØÈõªË©±'),

            # === ADDRESS (‰ΩèÊâÄÊÉÖÂ†±) ===
            postal_code=data.get('ÈÉµ‰æøÁï™Âè∑'),
            current_address=data.get('Áèæ‰ΩèÊâÄ'),
            address=data.get('Áèæ‰ΩèÊâÄ'),
            address_banchi=data.get('Áï™Âú∞'),
            address_building=data.get('Áâ©‰ª∂Âêç'),
            registered_address=data.get('ÁôªÈå≤‰ΩèÊâÄ'),

            # === PASSPORT & VISA („Éë„Çπ„Éù„Éº„Éà„Éª„Éì„Ç∂) ===
            passport_number=data.get('„Éë„Çπ„Éù„Éº„ÉàÁï™Âè∑'),
            passport_expiry=parse_date(data.get('„Éë„Çπ„Éù„Éº„ÉàÊúüÈôê')),
            residence_card_number=data.get('Âú®Áïô„Ç´„Éº„ÉâÁï™Âè∑'),
            residence_status=data.get('Âú®ÁïôË≥áÊ†º'),
            residence_expiry=parse_date(data.get('ÔºàÂú®Áïô„Ç´„Éº„ÉâË®òËºâÔºâÂú®ÁïôÊúüÈôê')),

            # === DRIVER'S LICENSE (ÈÅãËª¢ÂÖçË®±) ===
            license_number=data.get('ÈÅãËª¢ÂÖçË®±Áï™Âè∑Âèä„Å≥Êù°‰ª∂'),
            license_expiry=parse_date(data.get('ÈÅãËª¢ÂÖçË®±ÊúüÈôê')),
            car_ownership=data.get('Ëá™ÂãïËªäÊâÄÊúâ'),
            voluntary_insurance=data.get('‰ªªÊÑè‰øùÈô∫Âä†ÂÖ•'),

            # === QUALIFICATIONS (Ë≥áÊ†º„ÉªÂÖçË®±) ===
            forklift_license=data.get('ÔæåÔΩ´ÔΩ∞ÔΩ∏ÔæòÔæåÔæÑÂÖçË®±'),
            tama_kake=data.get('ÁéâÊéõ'),
            mobile_crane_under_5t=data.get('ÁßªÂãïÂºèÔΩ∏ÔæöÔΩ∞ÔæùÈÅãËª¢Â£´(5ÔæÑÔæùÊú™Ê∫Ä)'),
            mobile_crane_over_5t=data.get('ÁßªÂãïÂºèÔΩ∏ÔæöÔΩ∞ÔæùÈÅãËª¢Â£´(5ÔæÑÔæù‰ª•‰∏ä)'),
            gas_welding=data.get('ÔΩ∂ÔæûÔΩΩÊ∫∂Êé•‰ΩúÊ•≠ËÄÖ'),

            # === FAMILY (ÂÆ∂ÊóèÊßãÊàê) - Member 1 ===
            family_name_1=data.get('ÂÆ∂ÊóèÊßãÊàêÊ∞èÂêç1'),
            family_relation_1=data.get('ÂÆ∂ÊóèÊßãÊàêÁ∂öÊüÑ1'),
            family_age_1=parse_int(data.get('Âπ¥ÈΩ¢1')),
            family_residence_1=data.get('Â±Ö‰Ωè1'),
            family_separate_address_1=data.get('Âà•Â±Ö‰Ωè‰ΩèÊâÄ1'),

            # === FAMILY - Member 2 ===
            family_name_2=data.get('ÂÆ∂ÊóèÊßãÊàêÊ∞èÂêç2'),
            family_relation_2=data.get('ÂÆ∂ÊóèÊßãÊàêÁ∂öÊüÑ2'),
            family_age_2=parse_int(data.get('Âπ¥ÈΩ¢2')),
            family_residence_2=data.get('Â±Ö‰Ωè2'),
            family_separate_address_2=data.get('Âà•Â±Ö‰Ωè‰ΩèÊâÄ2'),

            # === FAMILY - Member 3 ===
            family_name_3=data.get('Ê∞èÂêç3'),
            family_relation_3=data.get('ÂÆ∂ÊóèÊßãÊàêÁ∂öÊüÑ3'),
            family_age_3=parse_int(data.get('Âπ¥ÈΩ¢3')),
            family_residence_3=data.get('Â±Ö‰Ωè3'),
            family_separate_address_3=data.get('Âà•Â±Ö‰Ωè‰ΩèÊâÄ3'),

            # === FAMILY - Member 4 ===
            family_name_4=data.get('ÂÆ∂ÊóèÊßãÊàêÊ∞èÂêç4'),
            family_relation_4=data.get('ÂÆ∂ÊóèÊßãÊàêÁ∂öÊüÑ4'),
            family_age_4=parse_int(data.get('Âπ¥ÈΩ¢4')),
            family_residence_4=data.get('Â±Ö‰Ωè4'),
            family_separate_address_4=data.get('Âà•Â±Ö‰Ωè‰ΩèÊâÄ4'),

            # === FAMILY - Member 5 ===
            family_name_5=data.get('ÂÆ∂ÊóèÊßãÊàêÊ∞èÂêç5'),
            family_relation_5=data.get('ÂÆ∂ÊóèÊßãÊàêÁ∂öÊüÑ5'),
            family_age_5=parse_int(data.get('Âπ¥ÈΩ¢5')),
            family_residence_5=data.get('Â±Ö‰Ωè5'),
            family_separate_address_5=data.get('Âà•Â±Ö‰Ωè‰ΩèÊâÄ5'),

            # === LANGUAGE SKILLS (Ë®ÄË™û„Çπ„Ç≠„É´ - actual model fields) ===
            can_speak=data.get('Êó•Êú¨Ë™ûË©±„Åõ„Çã'),
            can_understand=data.get('Êó•Êú¨Ë™ûËÅû„ÅçÂèñ„Çå„Çã'),
            can_read_kana=data.get('„Å≤„Çâ„Åå„Å™„Éª„Ç´„Çø„Ç´„ÉäË™≠„ÇÅ„Çã'),
            can_write_kana=data.get('„Å≤„Çâ„Åå„Å™„Éª„Ç´„Çø„Ç´„ÉäÊõ∏„Åë„Çã'),

            # === PHYSICAL (Ë∫´‰ΩìÊÉÖÂ†±) ===
            height=parse_float(data.get('Ë∫´Èï∑')),
            weight=parse_float(data.get('‰ΩìÈáç')),
            shoe_size=parse_float(data.get('Èù¥„ÅÆ„Çµ„Ç§„Ç∫')),
            waist=parse_int(data.get('„Ç¶„Ç®„Çπ„Éà')),
            blood_type=data.get('Ë°ÄÊ∂≤Âûã'),
            vision_left=parse_float(data.get('Ë¶ñÂäõÂ∑¶„ÄÄÂ∑¶')),
            vision_right=parse_float(data.get('Ë¶ñÂäõÂ∑¶„ÄÄÂè≥')),
            allergy_exists=data.get('„Ç¢„É¨„É´„ÇÆ„Éº„ÄÄÊúâ'),

            # === WORK EXPERIENCE (ÁµåÈ®ì‰ΩúÊ•≠ - Boolean fields) ===
            exp_nc_lathe=parse_bool(data.get('NCÊ©üÊ¢∞')),
            exp_painting=parse_bool(data.get('Â°óË£Ö')),
            exp_forklift=parse_bool(data.get('ÔæåÔΩ´ÔΩ∞ÔΩ∏ÔæòÔæåÔæÑ')),
            exp_packing=parse_bool(data.get('Ê¢±ÂåÖÔºàÈõªÂ≠êÔºâ')),
            exp_food_processing=parse_bool(data.get('È£üÂìÅË£ΩÈÄ†')),
            exp_line_leader=parse_bool(data.get('ÔæåÔΩ´ÔΩ∞ÔΩ∏ÔæòÔæåÔæÑÔæòÔΩ∞ÔæÄÔæûÔΩ∞')),
            exp_other=data.get('„Åù„ÅÆ‰ªñ'),

            # === COMMUTE (ÈÄöÂã§) ===
            commute_method=data.get('ÈÄöÂã§ÊñπÊ≥ï'),

            # === INTERVIEW (Èù¢Êé•„ÉªÊ§úÊüª) ===
            interview_result=data.get('Èù¢Êé•ÂæåÂç≥OK'),
            antigen_test_kit=data.get('ÈùûÂ∏∏Âè¨ÈõÜ„Ç≠„ÉÉ„Éà'),
            antigen_test_date=parse_date(data.get('ÈùûÂ∏∏Âè¨ÈõÜ„Ç≠„ÉÉ„ÉàÊï∞Èáè')),

            # === JAPANESE ABILITY (Êó•Êú¨Ë™ûËÉΩÂäõ) ===
            japanese_qualification=data.get('Êó•Êú¨Ë™ûËÉΩÂäõË≥áÊ†º'),
            japanese_level=data.get('„Çπ„Ç≠„É´Level'),

            # === EMERGENCY CONTACT (Á∑äÊÄ•ÈÄ£Áµ°ÂÖà) ===
            emergency_contact_name=data.get('Á∑äÊÄ•ÈÄ£Áµ°ÂÖà„ÄÄÊ∞èÂêç'),
            emergency_contact_relation=data.get('Á∑äÊÄ•ÈÄ£Áµ°ÂÖà„ÄÄÂêçÂâç'),
            emergency_contact_phone=data.get('Á∑äÊÄ•ÈÄ£Áµ°ÂÖà„ÄÄÈõªË©±Áï™Âè∑'),

            # === EDUCATION (Â≠¶Ê≠¥) ===
            major=data.get('ÊúÄÁµÇÂ≠¶Ê≠¥'),

            # === WORK HISTORY Entry 7 (only this one exists in model) ===
            work_history_company_7=data.get('ËÅ∑Ê≠¥Âπ¥Á§æÂêç7'),
            work_history_entry_company_7=data.get('ËÅ∑Ê≠¥ÂÖ•Á§æ‰ºöÁ§æÂêç7'),
            work_history_exit_company_7=data.get('ËÅ∑Ê≠¥ÈÄÄÁ§æ‰ºöÁ§æÂêç7'),

            # === STATUS ===
            status=CandidateStatus.PENDING,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )

        db.add(candidate)
        stats['imported'] += 1
        return True

    except Exception as e:
        stats['errors'] += 1
        logger.error(f"  Error importing {rirekisho_id}: {str(e)[:100]}")
        return False


def import_candidates_batch(json_file: str, batch_size: int = 100):
    """Import candidates in batches with progress reporting"""

    logger.info("=" * 80)
    logger.info("IMPORTING CANDIDATES WITH PHOTOS")
    logger.info("=" * 80)

    # Load JSON file
    logger.info(f"Loading JSON file: {json_file}")

    if not Path(json_file).exists():
        logger.error(f"File not found: {json_file}")
        return False

    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    candidates = data.get('candidates', [])
    total = len(candidates)

    logger.info(f"Total candidates to import: {total}")
    logger.info(f"Batch size: {batch_size}")
    logger.info("")

    # Statistics
    stats = {
        'imported': 0,
        'duplicates': 0,
        'errors': 0,
        'with_photo': 0
    }

    db = SessionLocal()
    start_time = datetime.now()

    try:
        for i, candidate_data in enumerate(candidates, 1):
            # Import candidate
            import_candidate(db, candidate_data, stats)

            # Commit in batches
            if i % batch_size == 0:
                db.commit()
                elapsed = (datetime.now() - start_time).total_seconds()
                rate = i / elapsed if elapsed > 0 else 0
                logger.info(f"  Progress: {i}/{total} ({i/total*100:.1f}%) - "
                          f"{stats['imported']} imported, {stats['with_photo']} with photos - "
                          f"{rate:.1f} records/sec")

        # Final commit
        db.commit()

        # Final report
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info("")
        logger.info("=" * 80)
        logger.info("IMPORT SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Total processed:     {total}")
        logger.info(f"‚úì Imported:          {stats['imported']}")
        logger.info(f"üì∏ With photos:       {stats['with_photo']}")
        logger.info(f"‚ö† Duplicates:        {stats['duplicates']}")
        logger.info(f"‚úó Errors:            {stats['errors']}")
        logger.info(f"‚è± Time elapsed:      {elapsed:.1f} seconds")
        logger.info(f"‚ö° Import rate:       {total/elapsed:.1f} records/sec")
        logger.info("=" * 80)

        # Verify in database
        logger.info("")
        logger.info("Verifying database records...")
        total_candidates = db.query(Candidate).count()
        candidates_with_photos = db.query(Candidate).filter(Candidate.photo_data_url.isnot(None)).count()
        logger.info(f"Total candidates in DB: {total_candidates}")
        logger.info(f"Candidates with photos: {candidates_with_photos}")
        logger.info("=" * 80)

        return True

    except Exception as e:
        db.rollback()
        logger.error(f"Fatal error during import: {e}")
        return False

    finally:
        db.close()


def main():
    """Main entry point"""
    json_file = '/app/config/all_candidates_with_photos.json'

    success = import_candidates_batch(json_file, batch_size=100)

    if success:
        logger.info("‚úÖ Import completed successfully")
        return 0
    else:
        logger.error("‚ùå Import failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())

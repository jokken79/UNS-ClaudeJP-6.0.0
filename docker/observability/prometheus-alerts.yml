groups:
  - name: service_availability
    interval: 30s
    rules:
      # Alert when a service is down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 1 minute."

      # Alert when backend is unreachable
      - alert: BackendDown
        expr: up{job="backend"} == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Backend service is down"
          description: "Backend FastAPI service has been unreachable for 30 seconds."

  - name: error_rates
    interval: 30s
    rules:
      # Alert when HTTP error rate is high
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.job }}."

      # Alert when HTTP 500 errors are occurring
      - alert: HTTP500Errors
        expr: rate(http_requests_total{status="500"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "HTTP 500 errors detected on {{ $labels.job }}"
          description: "Service {{ $labels.job }} is returning HTTP 500 errors."

  - name: performance
    interval: 30s
    rules:
      # Alert when response time is high (p95 > 1s)
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          ) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "P95 response time is {{ $value }}s on {{ $labels.job }}."

      # Alert when request rate is unusually high
      - alert: HighRequestRate
        expr: |
          sum(rate(http_requests_total[5m])) by (job) > 1000
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "High request rate on {{ $labels.job }}"
          description: "Request rate is {{ $value }} req/s on {{ $labels.job }}."

  - name: database
    interval: 30s
    rules:
      # Alert when database connection pool is near exhaustion
      - alert: DatabaseConnectionPoolHigh
        expr: |
          (
            db_pool_connections_active
            /
            db_pool_connections_max
          ) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool usage high"
          description: "Database connection pool is {{ $value | humanizePercentage }} full."

  - name: otel_collector
    interval: 30s
    rules:
      # Alert when OTel Collector is dropping data
      - alert: OTelCollectorDroppingData
        expr: rate(otelcol_processor_dropped_spans[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector is dropping spans"
          description: "OTel Collector is dropping {{ $value }} spans/s."

      # Alert when OTel Collector memory usage is high
      - alert: OTelCollectorHighMemory
        expr: |
          (
            process_resident_memory_bytes{job="otel-collector"}
            /
            1024 / 1024
          ) > 400
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector memory usage high"
          description: "OTel Collector is using {{ $value }}MB of memory."

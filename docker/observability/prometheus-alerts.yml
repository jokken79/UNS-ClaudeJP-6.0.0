# ========================================
# Prometheus Alerting Rules
# ========================================
#
# Purpose: Alert definitions for UNS-ClaudeJP monitoring
# Usage: Loaded automatically by Prometheus
#
# Author: Claude Code
# Created: 2025-11-12
# Version: 1.0.0
#
# ========================================

groups:
  # ========================================
  # Critical Alerts (Immediate Action)
  # ========================================
  - name: critical_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job=~"backend|frontend|postgres|redis|nginx"} == 0
        for: 1m
        labels:
          severity: critical
          component: "{{$labels.job}}"
        annotations:
          summary: "Service {{$labels.job}} is down"
          description: "Service {{$labels.job}} on {{$labels.instance}} has been down for more than 1 minute"
          runbook: "Check docker compose logs {{$labels.job}} and restart service if needed"

      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 30s
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL database has been unreachable for 30 seconds"
          runbook: "Immediate action required! Check database container and restart if necessary"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          component: backend
        annotations:
          summary: "High error rate detected"
          description: "Backend is returning more than 10% error responses (5xx) for the last 5 minutes"
          runbook: "Check backend logs for errors and investigate root cause"

      - alert: DatabaseConnectionPoolExhausted
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.9
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Database connection pool is at {{$value}}% capacity"
          runbook: "Scale backend instances or increase database max_connections"

      - alert: DiskSpaceCritical
        expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) > 0.90
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Disk space critically low"
          description: "Disk usage is above 90% on {{$labels.instance}}"
          runbook: "Clean up old logs, backups, or docker images immediately"

  # ========================================
  # High Priority Alerts (Action within 30 min)
  # ========================================
  - name: high_priority_alerts
    interval: 1m
    rules:
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 10m
        labels:
          severity: high
          component: backend
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is above 1 second: {{$value}}s"
          runbook: "Check backend performance, database queries, and consider scaling"

      - alert: MemoryUsageHigh
        expr: (container_memory_usage_bytes{name=~"uns-claudejp.*"} / container_spec_memory_limit_bytes{name=~"uns-claudejp.*"}) > 0.85
        for: 5m
        labels:
          severity: high
          component: "{{$labels.name}}"
        annotations:
          summary: "Container memory usage is high"
          description: "Container {{$labels.name}} is using {{$value}}% of allocated memory"
          runbook: "Investigate memory leaks or increase container memory limit"

      - alert: CPUUsageHigh
        expr: rate(container_cpu_usage_seconds_total{name=~"uns-claudejp.*"}[5m]) > 0.80
        for: 10m
        labels:
          severity: high
          component: "{{$labels.name}}"
        annotations:
          summary: "Container CPU usage is high"
          description: "Container {{$labels.name}} is using {{$value}}% CPU for 10 minutes"
          runbook: "Check for CPU-intensive operations or scale horizontally"

      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 2m
        labels:
          severity: high
          component: cache
        annotations:
          summary: "Redis cache is down"
          description: "Redis has been unreachable for 2 minutes"
          runbook: "Backend will fall back to database. Restart Redis container"

      - alert: BackendInstanceDown
        expr: count(up{job="backend"} == 1) < 2
        for: 5m
        labels:
          severity: high
          component: backend
        annotations:
          summary: "Backend high availability compromised"
          description: "Less than 2 backend instances are running (current: {{$value}})"
          runbook: "Scale backend to at least 2 instances for high availability"

  # ========================================
  # Warning Alerts (Monitor/Plan Action)
  # ========================================
  - name: warning_alerts
    interval: 2m
    rules:
      - alert: ResponseTimeWarning
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 15m
        labels:
          severity: warning
          component: backend
        annotations:
          summary: "Response time above acceptable threshold"
          description: "95th percentile response time is {{$value}}s (target: < 0.5s)"
          runbook: "Monitor performance and investigate if it continues to degrade"

      - alert: DatabaseConnectionsWarning
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.70
        for: 10m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database connection pool usage is high"
          description: "Database connection pool is at {{$value}}% capacity"
          runbook: "Plan to scale backend or increase connection pool size"

      - alert: DiskSpaceWarning
        expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) > 0.70
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Disk space running low"
          description: "Disk usage is above 70% on {{$labels.instance}}"
          runbook: "Plan to clean up old data or expand disk space"

      - alert: RedisCacheHitRateLow
        expr: (redis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total)) < 0.80
        for: 30m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis cache hit rate is low"
          description: "Cache hit rate is {{$value}}% (target: > 80%)"
          runbook: "Review cache strategy and TTL settings"

      - alert: HighAuthFailureRate
        expr: rate(auth_login_failed_total[5m]) / rate(auth_login_total[5m]) > 0.10
        for: 10m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High authentication failure rate"
          description: "{{$value}}% of login attempts are failing"
          runbook: "Check for brute force attacks or user login issues"

  # ========================================
  # Security Alerts
  # ========================================
  - name: security_alerts
    interval: 1m
    rules:
      - alert: FailedLoginSpike
        expr: increase(auth_login_failed_total[5m]) > 20
        for: 5m
        labels:
          severity: high
          component: security
        annotations:
          summary: "Spike in failed login attempts detected"
          description: "{{$value}} failed login attempts in the last 5 minutes"
          runbook: "Investigate for brute force attack. Check IP addresses and consider blocking"

      - alert: UnauthorizedAccessAttempts
        expr: increase(http_requests_total{status="401"}[5m]) > 50
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "Multiple unauthorized access attempts"
          description: "{{$value}} unauthorized (401) access attempts in 5 minutes"
          runbook: "Review access logs and investigate suspicious IPs"

      - alert: SQLInjectionAttempt
        expr: increase(security_sql_injection_attempts_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          component: security
        annotations:
          summary: "SQL injection attempt detected"
          description: "{{$value}} SQL injection attempts detected"
          runbook: "Immediate investigation required! Block offending IP and review logs"

      - alert: RateLimitViolations
        expr: increase(rate_limit_exceeded_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High rate of rate limit violations"
          description: "{{$value}} rate limit violations in the last 5 minutes"
          runbook: "Investigate potential DoS attack or misconfigured client"

  # ========================================
  # Business Metrics Alerts
  # ========================================
  - name: business_alerts
    interval: 5m
    rules:
      - alert: PendingRequestsHigh
        expr: requests_pending_total > 50
        for: 30m
        labels:
          severity: warning
          component: business
        annotations:
          summary: "High number of pending employee requests"
          description: "{{$value}} pending requests waiting for approval"
          runbook: "Notify coordinators to review pending requests"

      - alert: OCRFailureRateHigh
        expr: rate(ocr_requests_total{status="failed"}[1h]) / rate(ocr_requests_total[1h]) > 0.20
        for: 30m
        labels:
          severity: warning
          component: business
        annotations:
          summary: "High OCR failure rate"
          description: "{{$value}}% of OCR requests are failing"
          runbook: "Check OCR service credentials and image quality"

      - alert: NoNewCandidatesRecently
        expr: increase(candidates_created_total[24h]) == 0
        for: 2h
        labels:
          severity: info
          component: business
        annotations:
          summary: "No new candidates in the last 24 hours"
          description: "No candidate registrations detected in 24 hours"
          runbook: "This might be expected during weekends/holidays. Otherwise, check candidate portal"

  # ========================================
  # Infrastructure Health
  # ========================================
  - name: infrastructure_health
    interval: 2m
    rules:
      - alert: ContainerRestartingFrequently
        expr: increase(container_restarts_total{name=~"uns-claudejp.*"}[1h]) > 3
        for: 5m
        labels:
          severity: high
          component: "{{$labels.name}}"
        annotations:
          summary: "Container is restarting frequently"
          description: "Container {{$labels.name}} has restarted {{$value}} times in the last hour"
          runbook: "Check container logs for crashes and fix underlying issue"

      - alert: NetworkLatencyHigh
        expr: rate(container_network_receive_bytes_total{name=~"uns-claudejp.*"}[5m]) > 1073741824
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High network traffic detected"
          description: "Container {{$labels.name}} is receiving {{$value}} bytes/s"
          runbook: "Investigate unusual network activity"

      - alert: BackupJobFailed
        expr: time() - backup_last_success_timestamp > 86400
        for: 1h
        labels:
          severity: high
          component: backup
        annotations:
          summary: "Database backup has not completed in 24 hours"
          description: "Last successful backup was {{$value}} seconds ago"
          runbook: "Check backup service logs and ensure backups are running"

  # ========================================
  # Load Balancing Alerts
  # ========================================
  - name: load_balancing_alerts
    interval: 1m
    rules:
      - alert: BackendInstanceUnhealthy
        expr: count(up{job="backend"} == 0) > 0
        for: 2m
        labels:
          severity: high
          component: backend
        annotations:
          summary: "One or more backend instances are unhealthy"
          description: "{{$value}} backend instance(s) are not responding to health checks"
          runbook: "Check backend logs and health endpoint. Restart if necessary"

      - alert: UnbalancedTrafficDistribution
        expr: stddev(rate(http_requests_total{job="backend"}[5m])) / avg(rate(http_requests_total{job="backend"}[5m])) > 0.30
        for: 10m
        labels:
          severity: warning
          component: load_balancer
        annotations:
          summary: "Traffic distribution is unbalanced across backend instances"
          description: "Standard deviation of request rate is {{$value}}% of average"
          runbook: "Check nginx load balancing configuration and backend health"

# ========================================
# Alert Configuration Notes
# ========================================
#
# Severity Levels:
#   - critical: Immediate action required (< 5 minutes)
#   - high: Action required within 30 minutes
#   - warning: Action required within 4 hours
#   - info: Informational, no immediate action
#
# Runbook Links:
#   All alerts include a runbook annotation with suggested actions
#
# Integration:
#   Alerts can be sent to:
#   - Grafana (built-in)
#   - Email (via Alertmanager)
#   - Slack (via Alertmanager)
#   - PagerDuty (via Alertmanager)
#
# Testing Alerts:
#   curl -X POST http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={__name__=~".+"}
#
# ========================================
